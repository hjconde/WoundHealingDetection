

## 0. Main Environments
```bash
conda create -n CAE python=3.9
conda activate CAE
pip install torch==1.12.1
pip install torchvision==0.13.1
pip install numpy==1.24.4
pip install pandas==1.3.5
pip install scikit-learn==1.0.2
pip install pillow==9.0.1
pip install opencv-python==4.10.0.84
pip install opencv-python-headless==4.10.0.84
pip install openpyxl==3.1.5
pip install networkx==3.1
pip install argparse==1.1
```



## 1. Prepare the dataset


## Explanation Regarding Training and Testing Datasets
- After downloading the datasets, you are supposed to put them into './data/', and the file format reference is as follows. (take the Skin_MT dataset as an example.)

- './data/Skin_MT/'
  - trainA
    - .png
  - trainB
    - .png
  
  - testA
    - .png
  - testB
    - .png

- training images with normal (without wound) label will be put into 'trainA/' folder, while training images with abnormal (with wound) labels will be put into 'trainB/'
- test images with normal label will be put into 'testA/' folder, while test images with abnormal labels will be put into 'testB/'
- the names and the labels of the training images (with the format 'image_name label') are put into the 'trainAB_img-name_label.txt'
- the names and the labels of the test images (with the format 'image_name label') are put into the 'testAB_img-name_label.txt'

## 2. Train the CAE
Run the 'main_train.py' file, then the CAE model starts training
```bash
python main_train.py  # Train and test CAE model.
```

## 3. Obtain the trained CAE (GAN) models and generated results of some cases
- After trianing, you could obtain the trained models in '/results/models/'.

  The trained CAE models that are used for extracting wound-related features and generating new samples are named 'gen_index.pt' (where 'index' represents the iteration number of training).

- After trianing, you could obtain the generated results of some cases in '/results/images_display/'.

  In the '/results/images_display/' folder, for each image file, the first row is the original cases, the second row presents the samples generated by combining the wound-related features and the wound-unrelated features from the original cases, the third row is the donor samples whose wound-related are extracted, and the fourth row presents the samples generated by combining the wound-unrelated from the original samples in the first row with the wound-related codes from the samples in the third row.
  
  In the '/results/images_display/' folder, the file name, for example, 'gen_a2b_test2_00104000.jpg', means the generated results from A class (normal) to B class (abnormal) on test2 group samples using trained CAE model which has been trained 104000 iterations.


## 4. Perform wound-related features analysis on test datasets

Run the 'WR_features_extract.py' file, so we can extract wound-related features of the test images using trained CAE (GAN) model, which is put into the './trained_models/' folder.
The wound-related features extracted from the test dataset are put into the '/results/testAB_CL_codes_extraction_results.csv' file. Each feature consists of 8 values.
```bash
python WR_features_extract.py  # Extract wound-related features of the test images using trained models.
```

Run the 'tsne_analysis.py' file, so we can perform t-SNE analysis on extracted wound-related features.
The t-SNE analysis result is presented in the './results/tsne_analysis_result.png' file, where different numbers with different colors represent samples with different classes. 
```bash
python tsne_analysis.py  #  Perform t-SNE analysis on extracted class-associated codes.
```


## 5. Perform instance explanation using the wound-related space

Run the 'local_explanation_on_instance.py' file to generate saliency map for case explanation for black-box classifier.
Along the path from the example to the reference counter example, we obtain meaningful wound-related features for guided counterfactual generation, and by analyzing the changes of the generated samples and the changes of the outputs of the black-box model on the generated samples, we can get one saliency map for the instance explanation.
```bash
python local_explanation_on_instance.py  #  Along the guided path (from example to the goal counter example), we obtain meaningful wound-related features for guided counterfactual generation, and by analyzing the changes of the generated samples and the changes of the outputs of the black-box model on the generated samples, we can get one saliency map for the instance explanation. 
```


